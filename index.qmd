---
title: "Practice Work With Ollama in Python and R"
format: html
author: "Steven P. Sanderson II, MPH"
theme: darkly
date: today
toc: true
toc_depth: 2
---

```{r}
reticulate::py_require("ollama")
```
# Introduction

This is where I am going to learn how to use ollama with Python and R.

# Is ollama installed for python?
```{python}
import importlib.util
module_name = "ollama"

if importlib.util.find_spec(module_name):
    print(f"{module_name} is installed.")
else:
    print(f"{module_name} is not installed.")
```

# Simple Call to Ollama

Let's make a simple request using python to get a response to a simple question:

## Python

First in python.

```{python py_ex_1}
import ollama

response = ollama.list()

res = ollama.chat(
    model = "deepseek-v3.1:671b-cloud",
    messages = [
        {
            "role": "user",
            "content": "Write a haiku about Ollama and Deepseek"
        }
    ]
)

print(res)
print("\n")
print(res["message"]["content"])
```

## R

Now let's do the same but with R:

```{r r_ex_1, message = FALSE, warning = FALSE}
library(ollamar)

test_connection()

arg_list = list(
    model = "deepseek-v3.1:671b-cloud",
    prompt = "Write a haiku about Ollama and Deepseek."
)

response = do.call(generate, arg_list)
print(response)

resp_process(response, "text")
resp_process(response, "df")

# Or specify output type
txt <- do.call(generate, c(arg_list, output = "text"))
print(txt)
```

# Streaming

First in Pyhon

```{python py_ex_2_streaming}
response = ollama.list()

res = ollama.chat(
    model = "deepseek-v3.1:671b-cloud",
    messages = [
        {
            "role": "user",
            "content": "Why is the ocean so salty?"
        }
    ],
    stream=True
)

for chunk in res:
    print(chunk["message"]["content"], end="", flush = True)
```

## R

Now in R:

```{r r_ex_2_streaming, message = FALSE, warning = FALSE}
messages <- create_message("Why is the ocean so salty?")

arg_list = list(
    model = "deepseek-v3.1:671b-cloud",
    messages = messages,
    stream = TRUE,
    output = "text"
)

txt <- do.call(chat, arg_list)
print(txt)
```

# The Generate Endpoint

Now let's see how using generate changes things.

```{python py_ex_3_generate}
res = ollama.generate(
    model = "deepseek-v3.1:671b-cloud",
    prompt = "Why is the sky blue?"
)

# show
print(ollama.show("deepseek-v3.1:671b-cloud"))
```

Now in R

```{r r_ex_3_generate, message = FALSE, warning = FALSE}
messages <- create_message("Why is the ocean so salty? Be concise.")

arg_list = list(
    model = "deepseek-v3.1:671b-cloud",
    messages = messages,
    stream = TRUE,
    output = "text"
)

txt <- do.call(chat, arg_list)

# show
print(show("deepseek-v3.1:671b-cloud"))
```

# Using a MODELFILE

```{python py_ex_4_modelfile}
# The following use of modelfile in the python library ollama.create does not work.
modelfile = """
FROM gpt-oss:20b-cloud
SYSTEM You are a very smart assistant who knows everything about the oceans. You are very sussinct and informative.
PARAMETER temperature 0.1
"""

ollama.create(
    model = "knowitall",
    from_ = "gpt-oss:20b-cloud",
    system = "You are a very smart assitant who knows everyting about the oceans. You are very sussinct and informative.",
    parameters = {
        "temperature": 0.1
    }
)

res = ollama.generate(
    model = "knowitall",
    prompt = "Why is the ocean so salty?",
    format = "json"
)
print(res["response"])
```